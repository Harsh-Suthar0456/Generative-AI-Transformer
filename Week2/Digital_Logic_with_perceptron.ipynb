{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### All the required dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building a class for a Perceptron/Neuron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron:\n",
    "    def __init__(self, weights, bias):\n",
    "        self.weights = np.array(weights)\n",
    "        self.bias = bias\n",
    "\n",
    "    def predict(self, inputs):\n",
    "        inputs = np.array(inputs)\n",
    "        activation_sum = np.dot(inputs, self.weights.T) + self.bias\n",
    "        return self.activation_function(activation_sum)\n",
    "\n",
    "    def activation_function(self, value):\n",
    "        if(value>0):\n",
    "            return 1\n",
    "        else:\n",
    "            return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instantiating an object of Perceptron class to replicate the Digital Logic AND gate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: [0, 0]\t Output: 0\n",
      "Input: [1, 0]\t Output: 0\n",
      "Input: [0, 1]\t Output: 0\n",
      "Input: [1, 1]\t Output: 1\n"
     ]
    }
   ],
   "source": [
    "AND_gate = Perceptron(weights=[1,1], bias=-1.5)\n",
    "#### Testing the AND_gate with input values\n",
    "valid_inputs = [[0,0],\n",
    "                [1,0],\n",
    "                [0,1],\n",
    "                [1,1]]\n",
    "for input in valid_inputs:\n",
    "    print(f'Input: {input}\\t Output: {AND_gate.predict(input)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proof that we can't build a XOR gate with a single neuron that has a linear activation function\n",
    "XOR gate: <br>\n",
    "[0,0] -> 0 <br>\n",
    "[1,0] -> 1 <br>\n",
    "[0,1] -> 1 <br>\n",
    "[1,1] -> 0 <br>\n",
    "WLOG, we can assume the perceptron gets activated/returns 1 when the weighted sum is greater than 0, and 0 otherwise.\n",
    "Let the weights be [w<sub>1</sub> ,w<sub>2</sub>] and the bias be *b*. <br>\n",
    "Constructing 4 equations out of the input-output pairs: <br>\n",
    "*b* <= 0 (for [0,0] -> 0) <br>\n",
    "*b* + w<sub>1</sub> > 0 (for [1,0] -> 1) <br>\n",
    "*b* + w<sub>2</sub> > 0 (for [0,1] -> 1) <br>\n",
    "*b* + w<sub>1</sub> + w<sub>2</sub> <= 0 (for [1,1] -> 0) <br>\n",
    "\n",
    "It can be observed that these 4 equations are inconsistent, hence we can't assign the weights and biases of a single perceptron to replicate a XOR gate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a XOR gate using a hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: [0, 0]\t Output: 0\n",
      "Input: [1, 0]\t Output: 1\n",
      "Input: [0, 1]\t Output: 1\n",
      "Input: [1, 1]\t Output: 0\n"
     ]
    }
   ],
   "source": [
    "hidden_neuron_1 = Perceptron(weights = [1,1], bias = -0.5)\n",
    "hidden_neuron_2 = Perceptron(weights = [-1,-1], bias = 1.5)\n",
    "output_neuron = Perceptron(weights = [1,1], bias = -1.5)\n",
    "\n",
    "for input in valid_inputs:\n",
    "    hidden_1_output = hidden_neuron_1.predict(input)\n",
    "    hidden_2_output = hidden_neuron_2.predict(input)\n",
    "    xor_gate_output = output_neuron.predict([hidden_1_output, hidden_2_output])\n",
    "    print(f'Input: {input}\\t Output: {xor_gate_output}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building a multi-layer Perceptron, with one hidden layer and a final layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Two_Layer_Perceptron:\n",
    "    def __init__(self, hidden_layer_weights, hidden_layer_bias, output_layer_weights, output_layer_bias):\n",
    "        self.hidden_layer_weights = np.array(hidden_layer_weights)\n",
    "        self.hidden_layer_bias = np.array(hidden_layer_bias)\n",
    "        self.output_layer_weights = np.array(output_layer_weights)\n",
    "        self.output_layer_bias = output_layer_bias\n",
    "\n",
    "    def predict(self, inputs):\n",
    "        inputs = np.array(inputs)\n",
    "        hidden_layer_output = self.activation_function(np.dot(inputs, self.hidden_layer_weights.T) + self.hidden_layer_bias)\n",
    "        output_layer_output = self.activation_function(np.dot(hidden_layer_output, self.output_layer_weights.T) + self.output_layer_bias)\n",
    "        return output_layer_output\n",
    "    \n",
    "    def activation_function(self, value):\n",
    "        return value > 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Employing XOR gate in this new class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: [0, 0]\t Output: 0\n",
      "Input: [1, 0]\t Output: 1\n",
      "Input: [0, 1]\t Output: 1\n",
      "Input: [1, 1]\t Output: 0\n"
     ]
    }
   ],
   "source": [
    "XOR_gate= Two_Layer_Perceptron(hidden_layer_weights=[[1,1],\n",
    "                                                     [-1,-1]],\n",
    "                                hidden_layer_bias=[-0.5,1.5],\n",
    "                                output_layer_weights=[1,1],\n",
    "                                output_layer_bias=-1.5)\n",
    "for input in valid_inputs:\n",
    "    print(f'Input: {input}\\t Output: {int(XOR_gate.predict(input))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building a Full Adder using this new multi-layer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullAdder:\n",
    "    def __init__(self):\n",
    "        self.AND_gate = Perceptron(weights=[1,1], bias=-1.5)\n",
    "        self.OR_gate = Perceptron(weights=[1,1], bias=0)\n",
    "        self.XOR_gate = Two_Layer_Perceptron(hidden_layer_weights=[[1,1],\n",
    "                                                     [-1,-1]],\n",
    "                                hidden_layer_bias=[-0.5,1.5],\n",
    "                                output_layer_weights=[1,1],\n",
    "                                output_layer_bias=-1.5)\n",
    "        \n",
    "    def add(self, a, b, carry_in):\n",
    "        xor_a_b = self.XOR_gate.predict([a,b])\n",
    "        sum = self.XOR_gate.predict([xor_a_b,carry_in])\n",
    "        # sum=self.XOR_gate.predict([xor_a_b,xor_carry_a_b])\n",
    "\n",
    "        and_a_b = self.AND_gate.predict([a,b])\n",
    "        and_carry_xor_a_b = self.AND_gate.predict([carry_in, xor_a_b])\n",
    "        carry_out = self.OR_gate.predict([and_a_b, and_carry_xor_a_b])\n",
    "\n",
    "        return (sum, carry_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A: 0, B: 0, Carry_in: 0, Sum: 0, Carry_out: 0\n",
      "A: 0, B: 0, Carry_in: 1, Sum: 1, Carry_out: 0\n",
      "A: 0, B: 1, Carry_in: 0, Sum: 1, Carry_out: 0\n",
      "A: 0, B: 1, Carry_in: 1, Sum: 0, Carry_out: 1\n",
      "A: 1, B: 0, Carry_in: 0, Sum: 1, Carry_out: 0\n",
      "A: 1, B: 0, Carry_in: 1, Sum: 0, Carry_out: 1\n",
      "A: 1, B: 1, Carry_in: 0, Sum: 0, Carry_out: 1\n",
      "A: 1, B: 1, Carry_in: 1, Sum: 1, Carry_out: 1\n"
     ]
    }
   ],
   "source": [
    "for a in range(0,2):\n",
    "    for b in range(0,2):\n",
    "        for carry_in in range(0,2):\n",
    "            print(f'A: {a}, B: {b}, Carry_in: {carry_in}, Sum: {int(FullAdder().add(a,b,carry_in)[0])}, Carry_out: {int(FullAdder().add(a,b,carry_in)[1])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementing a Ripple Carry Adder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum: 547518066\n"
     ]
    }
   ],
   "source": [
    "a=547434634\n",
    "b=83432\n",
    "\n",
    "def binary_representation(number):\n",
    "    binary_string = bin(number)[2:]\n",
    "    if len(binary_string) < 64:\n",
    "        binary_string = '0' * (64 - len(binary_string)) + binary_string\n",
    "    return binary_string\n",
    "\n",
    "\n",
    "def ripple_carry_adder(a, b):\n",
    "    binary_a = binary_representation(a)\n",
    "    binary_b = binary_representation(b)\n",
    "\n",
    "    carry = 0\n",
    "    sum = 0\n",
    "    sum_binary = ''\n",
    "\n",
    "    for i in range(63, -1, -1):\n",
    "        bit_a = int(binary_a[i])\n",
    "        bit_b = int(binary_b[i])\n",
    "\n",
    "        sum, carry = FullAdder().add(bit_a, bit_b, carry)\n",
    "        sum=int(sum)\n",
    "        carry=int(carry)\n",
    "        sum_binary = str(sum) + sum_binary\n",
    "\n",
    "    sum_binary = str(carry) + sum_binary\n",
    "\n",
    "    return int(sum_binary, 2)\n",
    "\n",
    "print(f'Sum: {ripple_carry_adder(a, b)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
